{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847e301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"mohankrishnathalla/medical-insurance-cost-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e53d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>region</th>\n",
       "      <th>urban_rural</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>household_size</th>\n",
       "      <th>...</th>\n",
       "      <th>liver_disease</th>\n",
       "      <th>arthritis</th>\n",
       "      <th>mental_health</th>\n",
       "      <th>proc_imaging_count</th>\n",
       "      <th>proc_surgery_count</th>\n",
       "      <th>proc_physio_count</th>\n",
       "      <th>proc_consult_count</th>\n",
       "      <th>proc_lab_count</th>\n",
       "      <th>is_high_risk</th>\n",
       "      <th>had_major_procedure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80185</td>\n",
       "      <td>79</td>\n",
       "      <td>Female</td>\n",
       "      <td>North</td>\n",
       "      <td>Urban</td>\n",
       "      <td>12800.0</td>\n",
       "      <td>No HS</td>\n",
       "      <td>Married</td>\n",
       "      <td>Employed</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92992</td>\n",
       "      <td>53</td>\n",
       "      <td>Male</td>\n",
       "      <td>Central</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>89600.0</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>Married</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>76435</td>\n",
       "      <td>63</td>\n",
       "      <td>Female</td>\n",
       "      <td>North</td>\n",
       "      <td>Rural</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>HS</td>\n",
       "      <td>Single</td>\n",
       "      <td>Employed</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84005</td>\n",
       "      <td>36</td>\n",
       "      <td>Male</td>\n",
       "      <td>West</td>\n",
       "      <td>Rural</td>\n",
       "      <td>38900.0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Single</td>\n",
       "      <td>Employed</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80918</td>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>South</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>83700.0</td>\n",
       "      <td>HS</td>\n",
       "      <td>Single</td>\n",
       "      <td>Employed</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>87499</td>\n",
       "      <td>53</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>73500.0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>Married</td>\n",
       "      <td>Self-employed</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>6266</td>\n",
       "      <td>50</td>\n",
       "      <td>Male</td>\n",
       "      <td>West</td>\n",
       "      <td>Urban</td>\n",
       "      <td>127200.0</td>\n",
       "      <td>No HS</td>\n",
       "      <td>Married</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>54887</td>\n",
       "      <td>42</td>\n",
       "      <td>Male</td>\n",
       "      <td>East</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>21600.0</td>\n",
       "      <td>HS</td>\n",
       "      <td>Married</td>\n",
       "      <td>Employed</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>76821</td>\n",
       "      <td>41</td>\n",
       "      <td>Male</td>\n",
       "      <td>West</td>\n",
       "      <td>Rural</td>\n",
       "      <td>81900.0</td>\n",
       "      <td>HS</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>861</td>\n",
       "      <td>51</td>\n",
       "      <td>Female</td>\n",
       "      <td>South</td>\n",
       "      <td>Urban</td>\n",
       "      <td>43400.0</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>Single</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69917 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       person_id  age     sex   region urban_rural    income  education  \\\n",
       "1          80185   79  Female    North       Urban   12800.0      No HS   \n",
       "4          92992   53    Male  Central    Suburban   89600.0  Doctorate   \n",
       "5          76435   63  Female    North       Rural  305000.0         HS   \n",
       "6          84005   36    Male     West       Rural   38900.0    Masters   \n",
       "7          80918   21  Female    South    Suburban   83700.0         HS   \n",
       "...          ...  ...     ...      ...         ...       ...        ...   \n",
       "99992      87499   53    Male     East    Suburban   73500.0    Masters   \n",
       "99995       6266   50    Male     West       Urban  127200.0      No HS   \n",
       "99996      54887   42    Male     East    Suburban   21600.0         HS   \n",
       "99997      76821   41    Male     West       Rural   81900.0         HS   \n",
       "99998        861   51  Female    South       Urban   43400.0  Doctorate   \n",
       "\n",
       "      marital_status employment_status  household_size  ...  liver_disease  \\\n",
       "1            Married          Employed               3  ...              0   \n",
       "4            Married     Self-employed               2  ...              0   \n",
       "5             Single          Employed               3  ...              0   \n",
       "6             Single          Employed               1  ...              0   \n",
       "7             Single          Employed               3  ...              0   \n",
       "...              ...               ...             ...  ...            ...   \n",
       "99992        Married     Self-employed               2  ...              0   \n",
       "99995        Married          Employed               2  ...              0   \n",
       "99996        Married          Employed               2  ...              0   \n",
       "99997       Divorced        Unemployed               1  ...              0   \n",
       "99998         Single        Unemployed               3  ...              0   \n",
       "\n",
       "       arthritis mental_health proc_imaging_count  proc_surgery_count  \\\n",
       "1              1             1                  0                   0   \n",
       "4              1             0                  2                   0   \n",
       "5              0             0                  0                   0   \n",
       "6              0             0                  1                   0   \n",
       "7              0             0                  2                   1   \n",
       "...          ...           ...                ...                 ...   \n",
       "99992          0             0                  1                   0   \n",
       "99995          0             0                  0                   0   \n",
       "99996          0             0                  0                   0   \n",
       "99997          0             0                  1                   0   \n",
       "99998          0             1                  0                   0   \n",
       "\n",
       "       proc_physio_count  proc_consult_count  proc_lab_count  is_high_risk  \\\n",
       "1                      1                   0               1             1   \n",
       "4                      1                   1               0             1   \n",
       "5                      0                   0               1             1   \n",
       "6                      1                   0               1             0   \n",
       "7                      0                   0               1             0   \n",
       "...                  ...                 ...             ...           ...   \n",
       "99992                  1                   0               0             1   \n",
       "99995                  1                   0               0             0   \n",
       "99996                  0                   0               0             0   \n",
       "99997                  1                   0               0             0   \n",
       "99998                  2                   2               1             0   \n",
       "\n",
       "       had_major_procedure  \n",
       "1                        0  \n",
       "4                        0  \n",
       "5                        0  \n",
       "6                        0  \n",
       "7                        1  \n",
       "...                    ...  \n",
       "99992                    0  \n",
       "99995                    0  \n",
       "99996                    0  \n",
       "99997                    0  \n",
       "99998                    0  \n",
       "\n",
       "[69917 rows x 54 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(path + \"/medical_insurance.csv\")\n",
    "data.loc[1]\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c6011e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5714, 1.    , 0.2857, 0.8681, 0.6923, 0.1978, 0.3187, 0.8571,\n",
       "       0.7582, 0.4396, 0.1648, 0.3736, 0.7692, 0.0659, 0.3956, 0.4725,\n",
       "       0.1538, 0.5494, 0.7473, 0.8352, 0.2747, 0.3846, 0.7143, 0.4615,\n",
       "       0.9011, 0.5055, 0.5824, 0.4286, 0.3297, 0.978 , 0.3077, 0.7802,\n",
       "       0.5604, 0.3516, 0.5934, 0.2418, 0.4945, 0.4066, 0.8901, 0.1868,\n",
       "       0.4505, 0.3626, 0.6703, 0.6264, 0.5385, 0.8242, 0.011 , 0.1319,\n",
       "       0.4176, 0.2527, 0.3407, 0.0879, 0.2967, 0.1429, 0.6813, 0.4835,\n",
       "       0.8132, 0.5275, 0.2198, 0.1758, 0.9231, 0.2088, 0.9451, 0.6593,\n",
       "       0.9341, 0.6484, 0.6044, 0.5165, 0.6374, 0.7363, 0.    , 0.2637,\n",
       "       0.8022, 0.7253, 0.6154, 0.044 , 0.989 , 0.0989, 0.7033, 0.022 ,\n",
       "       0.9121, 0.8791, 0.033 , 0.8462, 0.2308, 0.956 , 0.967 , 0.0769,\n",
       "       0.0549, 0.1099, 0.7912, 0.1209])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['risk_score'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee31893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:14:10.418049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 17:14:10.444886: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 17:14:16.372701: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/ubuntu/code_servers/code-server/.venv/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765559659.499196  325094 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14039 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 17:14:23.565577: I external/local_xla/xla/service/service.cc:163] XLA service 0x742d4c002360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-12 17:14:23.565600: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-12-12 17:14:23.645474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-12 17:14:24.187042: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-12-12 17:14:24.298585: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298616: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298622: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298666: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298675: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298888: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298904: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298913: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298924: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298934: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298943: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298949: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298955: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.298964: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-12 17:14:24.612432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1367', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:24.677003: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1513', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:25.134713: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2313', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:25.189080: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1833', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:25.265642: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1673', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:25.281425: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1513', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:25.378775: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1513', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:26.614864: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2153', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:27.030542: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2473', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:27.850301: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3513', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:29.139444: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3909', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:29.377377: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4008', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:29.860279: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4107', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:30.014230: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4124', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:30.280013: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3810', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:30.923882: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4243', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:31.024054: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4192', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:31.462944: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4107', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:31.487361: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4209', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:31.707131: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4141', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:32.153714: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4226', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:32.750354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4158', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-12 17:14:32.754491: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4175', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  28/2000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 0.4090 - mean_absolute_error: 0.4148 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765559683.081784  334433 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 7ms/step - loss: 0.0058 - mean_absolute_error: 0.0446 - val_loss: 9.7990e-04 - val_mean_absolute_error: 0.0250\n",
      "Epoch 2/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 8.8445e-04 - mean_absolute_error: 0.0231 - val_loss: 8.7211e-04 - val_mean_absolute_error: 0.0233\n",
      "Epoch 3/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 7.8891e-04 - mean_absolute_error: 0.0214 - val_loss: 6.5287e-04 - val_mean_absolute_error: 0.0190\n",
      "Epoch 4/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 5.5665e-04 - mean_absolute_error: 0.0177 - val_loss: 6.7264e-04 - val_mean_absolute_error: 0.0208\n",
      "Epoch 5/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 4.1879e-04 - mean_absolute_error: 0.0150 - val_loss: 3.6793e-04 - val_mean_absolute_error: 0.0131\n",
      "Epoch 6/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - loss: 3.9464e-04 - mean_absolute_error: 0.0146 - val_loss: 4.6461e-04 - val_mean_absolute_error: 0.0164\n",
      "Epoch 7/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 3.4204e-04 - mean_absolute_error: 0.0136 - val_loss: 2.6325e-04 - val_mean_absolute_error: 0.0118\n",
      "Epoch 8/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 3.5275e-04 - mean_absolute_error: 0.0137 - val_loss: 2.6213e-04 - val_mean_absolute_error: 0.0119\n",
      "Epoch 9/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 2.7148e-04 - mean_absolute_error: 0.0120 - val_loss: 1.8286e-04 - val_mean_absolute_error: 0.0094\n",
      "Epoch 10/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 2.2060e-04 - mean_absolute_error: 0.0106 - val_loss: 1.0815e-04 - val_mean_absolute_error: 0.0065\n",
      "Epoch 11/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 1.8514e-04 - mean_absolute_error: 0.0095 - val_loss: 2.3337e-04 - val_mean_absolute_error: 0.0107\n",
      "Epoch 12/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - loss: 1.6779e-04 - mean_absolute_error: 0.0092 - val_loss: 3.3156e-04 - val_mean_absolute_error: 0.0150\n",
      "Epoch 13/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1.4146e-04 - mean_absolute_error: 0.0083 - val_loss: 1.6283e-04 - val_mean_absolute_error: 0.0090\n",
      "Epoch 14/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1.2646e-04 - mean_absolute_error: 0.0078 - val_loss: 1.5261e-04 - val_mean_absolute_error: 0.0093\n",
      "Epoch 15/15\n",
      "\u001b[1m2000/2000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1.1846e-04 - mean_absolute_error: 0.0074 - val_loss: 2.0702e-04 - val_mean_absolute_error: 0.0110\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 957us/step\n",
      "MAE: 0.011056173247488438, MSE: 0.00021227274977869283, RMSE: 0.014569583033796569\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = data.drop('risk_score', axis=1)\n",
    "y = data['risk_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=86)\n",
    "categorical_features = ['sex', 'region', 'urban_rural', 'education', 'marital_status', 'employment_status', 'smoker', 'alcohol_freq', 'plan_type', 'network_tier']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', StandardScaler(), numerical_features),\n",
    "\t\t('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "\t])\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=(input_dim,)))\n",
    "model.add(keras.layers.Dense(8192, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(4096, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(4096, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(2048, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(1024, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(512, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(32, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(16, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adamw', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")\n",
    "model.save(\"model.keras\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e7fd6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.save of <Sequential name=sequential, built=False>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "keras.Sequential().save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ba327",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "\tjson_file.write(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1d0f9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('txt/second/third')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "x = Path('txt/second/third/') / (\"asd\" + \".txt\")\n",
    "x.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e809751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0279577976484d7c8eccba349d45e020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving weights:   0%|          | 0/23 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 j: 8128\n",
      "[ 0.00656578  0.00551201 -0.01719118 -0.002198   -0.03010065  0.02942467\n",
      " -0.01573257  0.00706124  0.00073732  0.00568099  0.00311108 -0.06236957\n",
      "  0.01257644  0.00053292 -0.00013466  0.00553536  0.01443937 -0.01016005\n",
      "  0.00269679  0.00015261  0.0052608   0.00184306 -0.00685316  0.00115844\n",
      " -0.00643348  0.01115423 -0.00126962  0.00118736  0.00450148 -0.00237069\n",
      " -0.00300767  0.00248361]\n",
      "i: 0 j: 8128\n",
      "[153, 104, 91, 105, 137, 129, 152, 137, 167, 136, 152, 120, 121, 136, 152, 136]\n",
      "i: 0 j: 8160\n",
      "[ 1.7233782e-03  1.7817143e-03  8.5615240e-05 -1.0021025e-03\n",
      "  9.2019722e-02 -8.7909627e-04 -4.1221022e-03  1.2006492e-03\n",
      " -2.5204387e-03 -2.4301734e-02 -4.7882497e-03  5.5040186e-03\n",
      " -7.8835413e-03 -1.0640441e-02  4.6611127e-02 -5.3767613e-03\n",
      "  2.1192238e-03 -2.2610236e-02  7.2979141e-04 -2.0405655e-03\n",
      " -3.9625280e-02  6.2561349e-04 -2.8569305e-03  1.4675390e-03\n",
      " -1.0176206e-02  2.9928167e-03  7.1384045e-03 -7.6505472e-04\n",
      " -2.0220997e-02 -4.7451677e-03 -5.4470249e-03 -4.9842577e-03]\n",
      "i: 0 j: 8160\n",
      "[136, 136, 248, 136, 134, 136, 119, 200, 134, 136, 88, 136, 120, 152, 104, 136]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import struct\n",
    "import ctypes\n",
    "\n",
    "model = tf.keras.models.load_model(\"model.keras\")\n",
    "progress = tqdm(total=len(model.layers), desc=\"Saving weights\", unit=\"layer\")\n",
    "with open (\"quant.bin\", \"wb\") as f:\n",
    "\tfor layer in model.layers:\n",
    "\t\tfor weight in layer.weights:\n",
    "\t\t\tw = weight.numpy()\n",
    "\t\t\tif weight.name == 'kernel':\n",
    "\t\t\t\tfor i in range(w.shape[0]):\n",
    "\t\t\t\t\tfor j in range(0, w.shape[1], 32):\n",
    "\t\t\t\t\t\tw_block = w[i][j:j+32]\n",
    "\t\t\t\t\t\tif i == 0 and (j == 8160 or j == 8128):\n",
    "\t\t\t\t\t\t\tprint(f'i: {i} j: {j}\\n{w_block}')\n",
    "\t\t\t\t\t\tscale = (np.max(np.abs(w_block))) / 7\n",
    "\t\t\t\t\t\tif scale == 0:\n",
    "\t\t\t\t\t\t\tprint(\"ohno\")\n",
    "\t\t\t\t\t\tw_block_q = np.clip(np.round(w_block / scale), -7, 7).astype(np.int8)\n",
    "\t\t\t\t\t\tw_block_q = w_block_q + 8\n",
    "\t\t\t\t\t\tf.write(struct.pack('>f', scale))\n",
    "\t\t\t\t\t\tpacked = [ctypes.c_uint8((w_block_q[k] & 0x0F) << 4 | (w_block_q[k+1] & 0x0F) if k+1 < len(w_block_q) else 0).value for k in range(0, len(w_block_q), 2)]\n",
    "\t\t\t\t\t\tif i == 0 and (j == 8160 or j == 8128):\n",
    "\t\t\t\t\t\t\tprint(f'i: {i} j: {j}\\n{packed}')\n",
    "\t\t\t\t\t\tf.write(bytearray(packed))\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor i in range(0, w.shape[0], 32):\n",
    "\t\t\t\t\tw_block = w[i:i+32]\n",
    "\t\t\t\t\tscale = (np.max(np.abs(w_block))) / 7\n",
    "\t\t\t\t\tw_block_q = np.clip(np.round(w_block / scale), -7, 7).astype(np.int8)\n",
    "\t\t\t\t\tw_block_q = w_block_q + 8\n",
    "\t\t\t\t\tf.write(struct.pack('>f', scale))\n",
    "\t\t\t\t\tpacked = [ctypes.c_uint8((w_block_q[k] & 0x0F) << 4 | (w_block_q[k+1] & 0x0F) if k+1 < len(w_block_q) else 0).value for k in range(0, len(w_block_q), 2)]\n",
    "\t\t\t\t\tf.write(bytearray(packed))\n",
    "\t\tprogress.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "353d53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1784df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel (83, 8192)\n",
      "bias (8192,)\n",
      "gamma (8192,)\n",
      "beta (8192,)\n",
      "kernel (8192, 4096)\n",
      "bias (4096,)\n",
      "gamma (4096,)\n",
      "beta (4096,)\n",
      "kernel (4096, 4096)\n",
      "bias (4096,)\n",
      "gamma (4096,)\n",
      "beta (4096,)\n",
      "kernel (4096, 2048)\n",
      "bias (2048,)\n",
      "gamma (2048,)\n",
      "beta (2048,)\n",
      "kernel (2048, 1024)\n",
      "bias (1024,)\n",
      "gamma (1024,)\n",
      "beta (1024,)\n",
      "kernel (1024, 512)\n",
      "bias (512,)\n",
      "gamma (512,)\n",
      "beta (512,)\n",
      "kernel (512, 256)\n",
      "bias (256,)\n",
      "gamma (256,)\n",
      "beta (256,)\n",
      "kernel (256, 128)\n",
      "bias (128,)\n",
      "gamma (128,)\n",
      "beta (128,)\n",
      "kernel (128, 64)\n",
      "bias (64,)\n",
      "gamma (64,)\n",
      "beta (64,)\n",
      "kernel (64, 32)\n",
      "bias (32,)\n",
      "gamma (32,)\n",
      "beta (32,)\n",
      "kernel (32, 16)\n",
      "bias (16,)\n",
      "gamma (16,)\n",
      "beta (16,)\n",
      "kernel (16, 1)\n",
      "bias (1,)\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "\tfor weight in layer.weights:\n",
    "\t\tprint(weight.name, weight.shape)\n",
    "\n",
    "print(len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70926e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 77822330 bytes\n",
      "3d5610f0e188888889888888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71a69045456405b9c3e2ba84c37d31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/23 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n",
      "layer_data: 170  data_hex: 1  data_shift: 9  batches: 0  irreg_val: 1  d1: 16  d2: 1\n",
      "ohno\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import json\n",
    "import numpy as np\n",
    "import struct\n",
    "from tqdm.notebook import tqdm\n",
    "config_data = json.load(open('model.json'))\n",
    "weights = {}\n",
    "with open('quant.bin', 'rb') as f:\n",
    "\tbin_data = f.read().hex()\n",
    "\tprint(f\"Read {len(bin_data)} bytes\")\n",
    "\tprint(bin_data[:24])\n",
    "\tprogress = tqdm(total=len(config_data['config']['layers'])-1, desc=\"Loading weights\", unit=\"layer\", miniters=1)\n",
    "\tptr = 0\n",
    "\tfor layer in config_data['config']['layers']:\n",
    "\t\tlayer_data = []\n",
    "\t\tif layer['class_name'] == 'InputLayer':\n",
    "\t\t\tcontinue\n",
    "\t\tif layer['class_name'] == 'Dense':\n",
    "\t\t\td1 = layer['build_config']['input_shape'][1]\n",
    "\t\t\td2 = layer['config']['units']\n",
    "\t\t\tw = np.array([])\n",
    "\t\t\tbatches = d2 // 32\n",
    "\t\t\tlayer_data = bin_data[ptr:ptr+(((((d1+1)*d2)//32)*8) if d2>=32 else 0)+(8*(d1+1) if d2%32 != 0 else 0)+((d1+1)*(d2+(d2%2)))]\n",
    "\t\t\tfor i in range(d1):\n",
    "\t\t\t\tw2 = np.array([])\n",
    "\t\t\t\tptr_2 = (i*batches*40) + (i*(d2-(batches*32)+(d2%2))) + (i*(8 if (batches*32) < d2 else 0))\n",
    "\t\t\t\tif d2 >= 32:\n",
    "\t\t\t\t\tcount1 = 0\n",
    "\t\t\t\t\tcount2 = 0\n",
    "\t\t\t\t\tfor j in range(batches):\n",
    "\t\t\t\t\t\tscale_hex = layer_data[ptr_2+(j*40):(ptr_2+(j*40))+8]\n",
    "\t\t\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\t\t\tdata_hex = layer_data[(ptr_2+(j*40))+8:(ptr_2+(j*40))+40]\n",
    "\t\t\t\t\t\tfor k in range(0, 16):\n",
    "\t\t\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\t\t\tw2 = np.append(w2, n1 * scale)\n",
    "\t\t\t\t\t\t\tcount1 += 1\n",
    "\t\t\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\t\t\tw2 = np.append(w2, (n2 - 8) * scale)\n",
    "\t\t\t\t\t\t\t\tcount2 +=1\n",
    "\t\t\t\t\t\t\tif n1 +8 == 0 and n2 == 0:\n",
    "\t\t\t\t\t\t\t\tprint(f'i: {i} j:{j} true')\n",
    "\n",
    "\t\t\t\tif d2 % 32 != 0:\n",
    "\t\t\t\t\tirreg_val = d2 % 32\n",
    "\t\t\t\t\tscale_hex = layer_data[ptr_2+(batches*40):ptr_2+(batches*40)+8]\n",
    "\t\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\t\tdata_hex = layer_data[ptr_2+(batches*40)+8:ptr_2+(batches*40)+(8+irreg_val)]\n",
    "\t\t\t\t\tif irreg_val %2 == 1:\n",
    "\t\t\t\t\t\tprint(f'layer_data: {len(layer_data)}  data_hex: {len(data_hex)}  data_shift: {(batches*40)+(8+irreg_val)}  batches: {batches}  irreg_val: {irreg_val}  d1: {d1}  d2: {d2}')\n",
    "\n",
    "\t\t\t\t\tfor k in range(0, (irreg_val // 2) + (irreg_val % 2)):\n",
    "\t\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\t\tw2 = np.append(w2, n1 * scale)\n",
    "\t\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\t\tw2 = np.append(w2, (n2 - 8) * scale)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tprint('ohno')\n",
    "\t\t\t\tif len(w) == 0:\n",
    "\t\t\t\t\tw = w2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tw = np.vstack((w, w2))\n",
    "\n",
    "\t\t\tptr_2 = (d1*batches*40) + (d1*(d2-(batches*32)+(d2%2))) + (d1*(8 if (batches*32) < d2 else 0))\n",
    "\t\t\tw3 = np.array([])\n",
    "\t\t\tif (d2 >= 32):\n",
    "\t\t\t\tfor i in range(batches):\n",
    "\t\t\t\t\tscale_hex = layer_data[ptr_2+(i*40):ptr_2+(i*40)+8]\n",
    "\t\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\t\tdata_hex = layer_data[ptr_2+(i*40)+8:ptr_2+(i*40)+40]\n",
    "\t\t\t\t\tfor k in range(0, 16):\n",
    "\t\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\t\tw3 = np.append(w3, n1 * scale)\n",
    "\t\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\t\tw3 = np.append(w3, (n2 - 8) * scale)\n",
    "\t\t\tif d2 % 32 != 0:\n",
    "\t\t\t\tirreg_val = d2 % 32\n",
    "\t\t\t\tscale_hex = layer_data[ptr_2+(batches*40):ptr_2+(batches*40)+8]\n",
    "\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\tdata_hex = layer_data[ptr_2+(batches*40)+8:ptr_2+(batches*40)+(8+irreg_val)]\n",
    "\t\t\t\tfor k in range(0, (irreg_val // 2) + (irreg_val % 2)):\n",
    "\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\tw3 = np.append(w3, n1 * scale)\n",
    "\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\tw3 = np.append(w3, (n2 - 8) * scale)\n",
    "\n",
    "\t\t\tweights[layer['config']['name']] = [w, w3]\n",
    "\t\t\t\t\t\n",
    "\t\tif layer['class_name'] == 'LayerNormalization':\n",
    "\t\t\td1 = layer['build_config']['input_shape'][1]\n",
    "\t\t\tlayer_data = bin_data[ptr:ptr+(((d1//32)*8)+(8 if d1%32 != 0 else 0)+d1+(d1%2))*2]\n",
    "\t\t\tw = np.array([])\n",
    "\t\t\tbatches = d1 // 32\n",
    "\t\t\tptr_2 = 0\n",
    "\t\t\tif (d1 >= 32):\n",
    "\t\t\t\tfor i in range(d1//32):\n",
    "\t\t\t\t\tscale_hex = layer_data[i*40:(i*40)+8]\n",
    "\t\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\t\tdata_hex = layer_data[(i*40)+8:(i*40)+40]\n",
    "\t\t\t\t\tfor k in range(0, 16):\n",
    "\t\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\t\tw = np.append(w, n1 * scale)\n",
    "\t\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\t\tw = np.append(w, (n2 - 8) * scale)\n",
    "\t\t\tif d1 % 32 != 0:\n",
    "\t\t\t\tirreg_val = d1 % 32\n",
    "\t\t\t\tscale_hex = layer_data[batches*40:(batches*40)+8]\n",
    "\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\tdata_hex = layer_data[(batches*40)+8:(batches*40)+(8+irreg_val)]\n",
    "\t\t\t\tfor k in range(0, (irreg_val // 2) + (irreg_val % 2)):\n",
    "\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\tw = np.append(w, n1 * scale)\n",
    "\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\tw = np.append(w, (n2 - 8) * scale)\n",
    "\t\t\tptr_2 = batches*40 + (d1-(batches*32)+(d1%2)) + (8 if batches*32 < d1 else 0)\n",
    "\t\t\tw2 = np.array([])\n",
    "\t\t\tif (d1 >= 32):\n",
    "\t\t\t\tfor i in range(d1//32):\n",
    "\t\t\t\t\tscale_hex = layer_data[ptr_2+(i*40):ptr_2+(i*40)+8]\n",
    "\t\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\t\tdata_hex = layer_data[ptr_2+(i*40)+8:ptr_2+(i*40)+40]\n",
    "\t\t\t\t\tfor k in range(0, 16):\n",
    "\t\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\t\tw2 = np.append(w2, n1 * scale)\n",
    "\t\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\t\tw2 = np.append(w2, (n2 - 8) * scale)\n",
    "\t\t\t\tptr_2 += batches*40\n",
    "\t\t\tif d1 % 32 != 0:\n",
    "\t\t\t\tirreg_val = d1 % 32\n",
    "\t\t\t\tscale_hex = layer_data[ptr_2+(batches*40):ptr_2+(batches*40)+8]\n",
    "\t\t\t\tscale = struct.unpack('>f', bytes.fromhex(scale_hex))[0]\n",
    "\t\t\t\tdata_hex = layer_data[ptr_2+(batches*40)+8:ptr_2+(batches*40)+(8+irreg_val)]\n",
    "\t\t\t\tfor k in range(0, (irreg_val // 2) + (irreg_val % 2)):\n",
    "\t\t\t\t\tbyte = int(data_hex[k*2:(k*2)+2], 16)\n",
    "\t\t\t\t\tn1 = ((byte >> 4) & 0x0F) - 8\n",
    "\t\t\t\t\tw2 = np.append(w2, n1 * scale)\n",
    "\t\t\t\t\tn2 = (byte & 0x0F)\n",
    "\t\t\t\t\tif (n2 != 0):\n",
    "\t\t\t\t\t\tw2 = np.append(w2, (n2 - 8) * scale)\n",
    "\t\t\tweights[layer['config']['name']] = [w, w2]\n",
    "\n",
    "\t\tptr += len(layer_data)\n",
    "\t\t\t\n",
    "\t\tprogress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc89123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">688,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">33,558,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">16,781,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,390,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │       \u001b[38;5;34m688,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │        \u001b[38;5;34m16,384\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m33,558,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │    \u001b[38;5;34m16,781,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m8,390,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,772,901</span> (712.48 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m186,772,901\u001b[0m (712.48 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,257,633</span> (237.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,257,633\u001b[0m (237.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,515,268</span> (474.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m124,515,268\u001b[0m (474.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.saving import deserialize_keras_object\n",
    "import json\n",
    "config_data = json.load(open('model.json'))\n",
    "model = deserialize_keras_object(config_data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f9f601",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m      2\u001b[39m \tname = layer.name\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m layer.name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mweights\u001b[49m:\n\u001b[32m      4\u001b[39m \t\tlayer.set_weights(weights[name])\n",
      "\u001b[31mNameError\u001b[39m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "\tname = layer.name\n",
    "\tif layer.name in weights:\n",
    "\t\tlayer.set_weights(weights[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ede0192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 932us/step\n",
      "MAE: 0.8117777000375015, MSE: 0.873895114477833, RMSE: 0.9348235739848632\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "## Preprocessing\n",
    "#X = data.drop('risk_score', axis=1)\n",
    "#y = data['risk_score']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=86)\n",
    "#categorical_features = ['sex', 'region', 'urban_rural', 'education', 'marital_status', 'employment_status', 'smoker', 'alcohol_freq', 'plan_type', 'network_tier']\n",
    "#numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "#preprocessor = ColumnTransformer(\n",
    "#\ttransformers=[\n",
    "#\t\t('num', StandardScaler(), numerical_features),\n",
    "#\t\t('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "#\t])\n",
    "#X_train = preprocessor.fit_transform(X_train)\n",
    "#X_test = preprocessor.transform(X_test)\n",
    "#input_dim = X_train.shape[1]\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c129ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6bc18f5e1243f8a12857f949040ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing weights:   0%|          | 0/23 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing done. Quant saved to: model.uniq\n"
     ]
    }
   ],
   "source": [
    "from uniquant import quantize\n",
    "quantize(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1becbdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72d04e5f4504e24b05b7a3527f3b24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dequantizing weights:   0%|          | 0/23 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dequantizing done. Model returned from function. Also model saved to: dequant.keras\n"
     ]
    }
   ],
   "source": [
    "from uniquant import dequantize_save\n",
    "\n",
    "model = dequantize_save(\"model.uniq\", model_name='dequant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ef2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from uniquant import quantize, dequantize\n",
    "from tqdm.auto import tqdm\n",
    "keras.Sequential().save\n",
    "# Preprocessing\n",
    "X = data.drop('risk_score', axis=1)\n",
    "y = data['risk_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=86)\n",
    "categorical_features = ['sex', 'region', 'urban_rural', 'education', 'marital_status', 'employment_status', 'smoker', 'alcohol_freq', 'plan_type', 'network_tier']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', StandardScaler(), numerical_features),\n",
    "\t\t('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "\t])\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "input_dim = X_train.shape[1]\n",
    "# Build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=(input_dim,)))\n",
    "model.add(keras.layers.Dense(8192, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(4096, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(4096, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(2048, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(1024, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(512, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(32, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(16, activation='gelu'))\n",
    "model.add(keras.layers.LayerNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adamw', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")\n",
    "results.append({\"mae\": mae, \"mse\": mse, \"rmse\": rmse})\n",
    "model.save(\"model.keras\", overwrite=True)\n",
    "del(model)\n",
    "del(X_train)\n",
    "del(y_train)\n",
    "\n",
    "progress = tqdm(total=5, desc=\"Tests\", unit=\"test\", miniters=1, mininterval=0)\n",
    "for num in [32,64,128,16,8]:\n",
    "\tquantize(\"model.keras\", overwrite=True)\n",
    "\tmodel = dequantize(\"model.uniq\")\n",
    "\ty_pred = model.predict(X_test).flatten()\n",
    "\tmae = mean_absolute_error(y_test, y_pred)\n",
    "\tmse = mean_squared_error(y_test, y_pred)\n",
    "\trmse = np.sqrt(mse)\n",
    "\tprint(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")\n",
    "\tresults.append({\"mae\": mae, \"mse\": mse, \"rmse\": rmse})\n",
    "\tdel model\n",
    "\tprogess.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53010f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 20:08:55.281491: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-12 20:08:55.319579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-12 20:08:56.087348: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1765570136.870309  405475 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21758 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad3b85cd2ee46e79f98e952b204b514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing weights:   0%|          | 0/23 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from uniquant import quantize\n",
    "quantize(\"model.keras\", overwrite=True, pack_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
